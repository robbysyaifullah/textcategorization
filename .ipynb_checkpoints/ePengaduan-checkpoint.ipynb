{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitterscraper import query_tweets\n",
    "import codecs, json, csv\n",
    "import pandas as pd\n",
    "import random, re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv():\n",
    "    with codecs.open(\"input/twitter_test.json\", 'r', 'utf-8') as f:\n",
    "        tweets = json.load(f, encoding='utf-8')\n",
    "\n",
    "    list_tweets = [list(elem.values()) for elem in tweets]\n",
    "    list_columns = list(tweets[0].keys())\n",
    "    df = pd.DataFrame(list_tweets, columns=list_columns)\n",
    "\n",
    "    list_tweet = []\n",
    "    for i in range(len(tweets)):\n",
    "        list_tweet.append([tweets[i]['user'], tweets[i]['text']])\n",
    "\n",
    "    df = pd.DataFrame(list_tweet, columns=['name', 'text'])\n",
    "    df.to_csv(\"output/twitter_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(data_size):\n",
    "    twitters = pd.read_csv(\"output/twitter_test.csv\")\n",
    "    username = twitters['name']\n",
    "    tweets = twitters['text']\n",
    "\n",
    "    return zip(*random.sample(list(zip(username, tweets)), data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pos_tag():\n",
    "    clf = joblib.load('model/pos_tagger.joblib')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_vectorizer():\n",
    "    clf = joblib.load('model/dict_vectorizer.joblib')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classifier():\n",
    "    clf = joblib.load('model/classifier.joblib')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_count_vectorizer():\n",
    "    clf = joblib.load('model/count_vectorizer.joblib')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twitter(raw):\n",
    "    #remove hashtag, link, and @\n",
    "    cleanr = re.compile(\"http?:\\/\\/.*[\\r\\n]*\")\n",
    "    cleantext = re.sub(cleanr, '', raw)\n",
    "    \n",
    "    cleanr = re.compile(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)*\")\n",
    "    cleantext = re.sub(cleanr, '', cleantext)\n",
    "    \n",
    "    cleantext = re.sub(cleanr, '', cleantext)\n",
    "    \n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(w):\n",
    "    words = word_tokenize(w)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pos_tag(X):\n",
    "    preprocess1 = []\n",
    "    preprocess2 = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        preprocess1.append(preprocess_twitter(X[i]))\n",
    "    \n",
    "    for i in range(len(preprocess1)):\n",
    "        preprocess2.append(tokenizer(preprocess1[i]))\n",
    "    \n",
    "    return preprocess2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbor_word(sentence, i):\n",
    "    if i == 0 :\n",
    "        prev_word = ''\n",
    "    else :\n",
    "        prev_word = sentence[i-1]\n",
    "    if i == len(sentence)-1 :\n",
    "        next_word = ''\n",
    "    else :\n",
    "        next_word = sentence[i+1]\n",
    "    return {'prev_word' : prev_word, 'next_word' : next_word}\n",
    "\n",
    "def morphems(word):\n",
    "    prefix_1 = word[0]\n",
    "    prefix_2 = word[:2]\n",
    "    prefix_3 = word[:3]\n",
    "    prefix_4 = word[:4]\n",
    "    suffix_1 = word[-1]\n",
    "    suffix_2 = word[-2:]\n",
    "    suffix_3 = word[-3:]\n",
    "\n",
    "    return {'prefix_1' : prefix_1, 'prefix_2' : prefix_2, 'prefix_3' : prefix_3, 'prefix_4' : prefix_4, 'suffix_1' : suffix_1, 'suffix_2' : suffix_2, 'suffix_3' : suffix_3}\n",
    "\n",
    "def has_hyphen(word):\n",
    "    return {'has_hyphen' : '-' in word}\n",
    "\n",
    "def is_digit(word):\n",
    "    return {'is_digit' : word.isdigit()}\n",
    "\n",
    "def word_case(word):\n",
    "    is_capitalized = word[0].upper() == word[0]\n",
    "    return {'is_capitalized' : is_capitalized}\n",
    "\n",
    "\n",
    "def word_position(sentence, index):\n",
    "    if (index == 0):\n",
    "        pos = 0\n",
    "        prev_pos = -1\n",
    "        next_pos = 1\n",
    "    elif (index == len(sentence)):\n",
    "        pos = 2\n",
    "        prev_pos = 1\n",
    "        next_pos = -1\n",
    "    else :\n",
    "        if(index == 1):\n",
    "            prev_pos = 0\n",
    "            pos = 1\n",
    "            next_pos = 2\n",
    "        elif(index == len(sentence)-1):\n",
    "            prev_pos = 1\n",
    "            pos = 2\n",
    "            next_pos = -1\n",
    "        else :\n",
    "            prev_pos = pos = next_pos = 1\n",
    "    return {'prev_pos' : prev_pos, 'pos' : pos, 'next_pos' : next_pos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(sentences):\n",
    "    X = []\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            features = {}\n",
    "\n",
    "            word = sentence[i]\n",
    "            features.update({'value': word})\n",
    "            \n",
    "            features.update(neighbor_word(sentence, i))\n",
    "            features.update(word_position(sentence, i))\n",
    "            features.update(morphems(word))\n",
    "            features.update(word_case(word))\n",
    "            features.update(has_hyphen(word))\n",
    "            features.update(is_digit(word))\n",
    "            \n",
    "            X.append(features)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_features(features):\n",
    "    dict_vect = load_dict_vectorizer()\n",
    "    features_v = dict_vect.transform(features)\n",
    "    return features_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairing_tag(tweets, tags):\n",
    "    j = 0\n",
    "    pair_tweet = []\n",
    "    for tweet in tweets:\n",
    "        pair = []\n",
    "        for i in range(len(tweet)):\n",
    "            pair.append((tweet[i], tags[j]))\n",
    "            j = j + 1\n",
    "        pair_tweet.append(pair)\n",
    "    return pair_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemWord(words):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "    sentence = ''\n",
    "    for word in words:\n",
    "        sentence += str(stemmer.stem(word)) + ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_classifier(X):\n",
    "    preprocess1 = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        preprocess1.append(stemWord(X[i]))\n",
    "    \n",
    "    return preprocess1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vectorizer(X):\n",
    "    count_vect = load_count_vectorizer()\n",
    "    X = count_vect.transform(X)\n",
    "    return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairing_class(username, tweets, y):\n",
    "    j = 0\n",
    "    pair = []\n",
    "    for tweet in tweets:\n",
    "        pair.append((username[j], tweet, y[j]))\n",
    "        j = j + 1\n",
    "    \n",
    "    return pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairing_all(pengaduan, paired_tweets):\n",
    "    j = 0\n",
    "    pair = []\n",
    "    for tweet in pengaduan:\n",
    "        pair.append((tweet, paired_tweets[j]))\n",
    "        j = j + 1\n",
    "    \n",
    "    return pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_to_csv()\n",
    "\n",
    "username, tweets = load_csv(10)\n",
    "\n",
    "pre_tweets = preprocess_pos_tag(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_tweets = preprocess_classifier(pre_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'username': 'Generasi_Kopi', 'Tindakan': ['dipecat', 'ditangkap'], 'Nominal': ['satu ini'], 'Keterangan': ['Tolong', 'Pak', 'manusia', 'sbg', 'PNS', 'Sy', 'tdk', 'rela', 'uang', 'rakyat', 'bayar', 'gaji', 'si', 'brengsek'], 'tweet_lengkap': 'Tolong Pak @jokowi & @DivHumasPolri agar manusia satu ini dipecat sbg PNS & ditangkap. Sy tdk rela uang rakyat bayar gaji si brengsek ini..https://twitter.com/wakilgubernurKW/status/848509715034152961\\xa0…'}\n",
      "username \t: Generasi_Kopi\n",
      "Tindakan \t: ['dipecat', 'ditangkap']\n",
      "Nominal \t: ['satu ini']\n",
      "Keterangan \t: ['Tolong', 'Pak', 'manusia', 'sbg', 'PNS', 'Sy', 'tdk', 'rela', 'uang', 'rakyat', 'bayar', 'gaji', 'si', 'brengsek']\n",
      "tweet_lengkap \t: Tolong Pak @jokowi & @DivHumasPolri agar manusia satu ini dipecat sbg PNS & ditangkap. Sy tdk rela uang rakyat bayar gaji si brengsek ini..https://twitter.com/wakilgubernurKW/status/848509715034152961 …\n"
     ]
    }
   ],
   "source": [
    "vector_tweets = word_vectorizer(prep_tweets)\n",
    "classifier = load_classifier()\n",
    "\n",
    "y = classifier.predict(vector_tweets)\n",
    "paired_class = pairing_class(username, tweets, y)\n",
    "paired_class\n",
    "\n",
    "pengaduan_list = []\n",
    "pengaduan_tweet = []\n",
    "\n",
    "for each in paired_class:\n",
    "    if (each[2]==1):\n",
    "        pengaduan_list.append(each)\n",
    "        pengaduan_tweet.append(each[1])\n",
    "if len (pengaduan_list) == 0:\n",
    "    print(\"Tidak terdeteksi pengaduan\")\n",
    "    for each in paired_class:\n",
    "        print(each)\n",
    "else:\n",
    "    pre_tweets = preprocess_pos_tag(pengaduan_tweet)\n",
    "    \n",
    "    featured_tweets = feature_extractor(pre_tweets)\n",
    "    X = vectorize_features(featured_tweets)\n",
    "    posTagger = load_pos_tag()\n",
    "\n",
    "    tags = posTagger.predict(X)\n",
    "    paired_tweets = pairing_tag(pre_tweets, tags)\n",
    "    pair = pairing_all(pengaduan_list, paired_tweets)\n",
    "    detection = {}\n",
    "    for each in pair:\n",
    "        Noun = []\n",
    "        Verb = []\n",
    "        Angka = []\n",
    "        for i in range(len(each[1])):\n",
    "            if ((each[1][i][1]=='NN') |( each[1][i][1]=='NNP' )| (each[1][i][1]=='NND') ):\n",
    "                Noun.append(each[1][i][0])\n",
    "            elif (each[1][i][1]=='VB' ):\n",
    "                Verb.append(each[1][i][0])\n",
    "            elif ((each[1][i][1]=='CD' )| (each[1][i][1]=='OD') ):\n",
    "                Angka.append(each[1][i][0] + ' ' + each[1][i+1][0])\n",
    "        aduan = ({ 'username' : each[0][0], \n",
    "                     'Tindakan':Verb, 'Nominal': Angka,\n",
    "                     'Keterangan' : Noun, 'tweet_lengkap':each[0][1]})\n",
    "        print\n",
    "    print( detection)\n",
    "    for key, val in detection.items():\n",
    "        print (key ,'\\t:', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, json, csv\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import pandas as pd, csv\n",
    "import string as string\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterscraper \"(Presiden OR Menteri OR Pemerintahan OR Jokowi) AND (Tolong OR Mohon)\" -l 100 -p 400 -bd 2017-01-01 -ed 2017-06-01 -o dengantolong2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('dengantolong2.json', 'r', 'utf-8') as f:\n",
    "    tweets = json.load(f, encoding='utf-8')\n",
    "\n",
    "list_tweets = [list(elem.values()) for elem in tweets]\n",
    "list_columns = list(tweets[0].keys())\n",
    "df = pd.DataFrame(list_tweets, columns=list_columns)\n",
    "df.to_csv('dengantolong.csv')\n",
    "tweets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b19599f729a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlist_tweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mlist_tweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_tweet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets' is not defined"
     ]
    }
   ],
   "source": [
    "list_tweet = []\n",
    "for i in range(len(tweets)):\n",
    "    list_tweet.append([tweets[i]['user'],'1', tweets[i]['text']])\n",
    "    \n",
    "df = pd.DataFrame(list_tweet, columns=['name','class','text'])\n",
    "# df.to_csv('dengantolong.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'untuk', 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'u', 'prefix-2': 'un', 'prefix-3': 'unt', 'prefix-4': 'untu', 'suffix-1': 'k', 'suffix-2': 'uk', 'suffix-3': 'tuk', 'prev_word': 'Kera', 'next_word': 'amankan', 'has_hyphen': False}\n",
      "Accuracy Score Logistic :  0.9204\n",
      "Accuracy 0.9204\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "def openFile(file_path) :\n",
    "    with open(file_path, 'r', encoding='utf-8') as tsvfile :\n",
    "        lines = tsvfile.read().split('\\n')\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def preprocessData(lines) :\n",
    "    pasangan = []\n",
    "    all_pasangan = []\n",
    "\n",
    "    for line in lines :\n",
    "        if line == '':\n",
    "            all_pasangan.append(pasangan)\n",
    "            pasangan = []\n",
    "        else :\n",
    "            kata, tag = line.split('\\t')\n",
    "            p = (kata, tag)\n",
    "            pasangan.append(p)\n",
    "\n",
    "    return all_pasangan\n",
    "\n",
    "def countConsonant(word) :\n",
    "    consonant = 0\n",
    "    for letter in word :\n",
    "        if letter in 'bcdfghjklmnpqrstvwxyz' :\n",
    "            consonant += 1\n",
    "    return consonant\n",
    "\n",
    "def counVocal(word) :\n",
    "    vocal = 0\n",
    "    for letter in word :\n",
    "        if letter in 'aiueo' :\n",
    "            vocal += 1\n",
    "\n",
    "    return vocal\n",
    "\n",
    "def extract_feature(sentence, index) :\n",
    "    word = sentence[index][0]\n",
    "    return {\n",
    "        'word': word,\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,\n",
    "        'is_all_lower': word.lower() == word,\n",
    "        'prefix-1': word[0],\n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "        'prefix-4': word[:4],\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1][0],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1][0],\n",
    "        'has_hyphen': '-' in word,\n",
    "    }    \n",
    "\n",
    "def splitTrainTest(data) :\n",
    "    cutoff = int(.80 * len(data))\n",
    "    training_sentences = data[:cutoff]\n",
    "    test_sentences = data[cutoff:]\n",
    "\n",
    "    return training_sentences, test_sentences\n",
    "\n",
    "def data_to_dataframe(datas) :\n",
    "    X = []\n",
    "    y = []\n",
    "    for data in datas :\n",
    "        for i in range(len(data)) :\n",
    "            X.append(extract_feature(data, i))\n",
    "            y.append(data[i][1])\n",
    "    \n",
    "    return X[:15000], y[:15000]\n",
    "\n",
    "def train_accuracy() :\n",
    "    lines = openFile('Indonesian_Manually_Tagged_Corpus.tsv')\n",
    "    sentences = preprocessData(lines)\n",
    "    train, test = splitTrainTest(sentences)\n",
    "\n",
    "    X, y = data_to_dataframe(train)\n",
    "    X_test, y_test = data_to_dataframe(test)\n",
    "    print (X[1])\n",
    "    v = DictVectorizer(sparse=False)\n",
    "    X_vect = v.fit_transform(X)\n",
    "    pickle.dump(v, open(\"post_tag_vectorizer.sav\", \"wb\"))\n",
    "    X_test_vect = v.transform(X_test)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_vect, y)\n",
    "    \n",
    "    model_pred = model.predict(X_test_vect)\n",
    "    pickle.dump(model, open(\"post_tag_model.sav\", \"wb\"))\n",
    "    print('Accuracy Score Logistic : ', accuracy_score(model_pred, y_test))\n",
    "    \n",
    "    return accuracy_score(model_pred, y_test)\n",
    "\n",
    "print('Accuracy', train_accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Indonesian_Manually_Tagged_Corpus.tsv', 'r', encoding='utf-8') as tsvfile :\n",
    "    rows = tsvfile.read().split('\\n')\n",
    "    \n",
    "pair = []\n",
    "sentences = []\n",
    "\n",
    "for each in rows:\n",
    "    if each == '':\n",
    "        sentences.append(pair)\n",
    "        pair = [] \n",
    "    else:\n",
    "        word, tag = each.split('\\t')\n",
    "        pair.append([word, tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbor_word(sentence, i):\n",
    "    if i == 0 :\n",
    "        prev_word = ''\n",
    "    else :\n",
    "        prev_word = sentence[i-1][0]\n",
    "    if i == len(sentence)-1 :\n",
    "        next_word = ''\n",
    "    else :\n",
    "        next_word = sentence[i+1][0]\n",
    "    return {'prev_word' : prev_word, 'next_word' : next_word}\n",
    "\n",
    "def morphems(word):\n",
    "    prefix_1 = word[0]\n",
    "    prefix_2 = word[:2]\n",
    "    prefix_3 = word[:3]\n",
    "    prefix_4 = word[:4]\n",
    "    suffix_1 = word[-1]\n",
    "    suffix_2 = word[-2:]\n",
    "    suffix_3 = word[-3:]\n",
    "\n",
    "    return {'prefix_1' : prefix_1, 'prefix_2' : prefix_2, 'prefix_3' : prefix_3, 'prefix_4' : prefix_4, 'suffix_1' : suffix_1, 'suffix_2' : suffix_2, 'suffix_3' : suffix_3}\n",
    "\n",
    "def has_hyphen(word):\n",
    "    return {'has_hyphen' : '-' in word}\n",
    "\n",
    "def is_numeric(word):\n",
    "    return {'is_numeric' : word.isdigit()}\n",
    "\n",
    "def word_case(word):\n",
    "    is_uppercase = word.upper() == word\n",
    "    is_lowercase = word.lower() == word\n",
    "    is_capitalized = word[0].upper() == word[0]\n",
    "    return {'is_uppercase' : is_uppercase,'is_lowercase' : is_lowercase, \n",
    "            'is_capitalized' : is_capitalized}\n",
    "\n",
    "\n",
    "def word_position(sentence, index):\n",
    "    if (index == 0):\n",
    "        pos = -1\n",
    "    elif (index == len):\n",
    "        pos = 1\n",
    "    else :\n",
    "        pos = 0\n",
    "    return {'pos' : pos}\n",
    "\n",
    "def feature_extractor(sentences):\n",
    "    X = []\n",
    "    y = []\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            features = {}\n",
    "\n",
    "            word = sentence[i][0]\n",
    "            features.update({'value': word})\n",
    "            \n",
    "            features.update(neighbor_word(sentence, i))\n",
    "            features.update(word_position(sentence, i))\n",
    "            features.update(morphems(word))\n",
    "            features.update(word_case(word))\n",
    "            features.update(has_hyphen(word))\n",
    "            features.update(is_numeric(word))\n",
    "            \n",
    "            X.append(features)\n",
    "            y.append(sentence[i][1])\n",
    "    return X, y\n",
    "X, y = feature_extractor(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-bd035e01f8fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdict_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "dict_vect = DictVectorizer(sparse=False)\n",
    "X = dict_vect.fit_transform(X_train[0:50000])\n",
    "\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

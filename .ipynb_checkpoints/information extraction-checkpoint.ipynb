{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import codecs, json, csv\n",
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "# import pandas as pd, csv\n",
    "# import string as string\n",
    "\n",
    "# import re\n",
    "\n",
    "# from nltk.stem import SnowballStemmer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitterscraper \"(Presiden OR Menteri OR Pemerintahan OR Jokowi) AND (Tolong OR Mohon)\" -l 100 -p 400 -bd 2017-01-01 -ed 2017-06-01 -o dengantolong2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with codecs.open('dengantolong2.json', 'r', 'utf-8') as f:\n",
    "#     tweets = json.load(f, encoding='utf-8')\n",
    "\n",
    "# list_tweets = [list(elem.values()) for elem in tweets]\n",
    "# list_columns = list(tweets[0].keys())\n",
    "# df = pd.DataFrame(list_tweets, columns=list_columns)\n",
    "# df.to_csv('dengantolong.csv')\n",
    "# tweets[1]\n",
    "\n",
    "# list_tweet = []\n",
    "# for i in range(len(tweets)):\n",
    "#     list_tweet.append([tweets[i]['user'],'1', tweets[i]['text']])\n",
    "    \n",
    "# df = pd.DataFrame(list_tweet, columns=['name','class','text'])\n",
    "# # df.to_csv('dengantolong.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import pickle\n",
    "# def openFile(file_path) :\n",
    "#     with open(file_path, 'r', encoding='utf-8') as tsvfile :\n",
    "#         lines = tsvfile.read().split('\\n')\n",
    "    \n",
    "#     return lines\n",
    "\n",
    "# def preprocessData(lines) :\n",
    "#     pasangan = []\n",
    "#     all_pasangan = []\n",
    "\n",
    "#     for line in lines :\n",
    "#         if line == '':\n",
    "#             all_pasangan.append(pasangan)\n",
    "#             pasangan = []\n",
    "#         else :\n",
    "#             kata, tag = line.split('\\t')\n",
    "#             p = (kata, tag)\n",
    "#             pasangan.append(p)\n",
    "\n",
    "#     return all_pasangan\n",
    "\n",
    "# def countConsonant(word) :\n",
    "#     consonant = 0\n",
    "#     for letter in word :\n",
    "#         if letter in 'bcdfghjklmnpqrstvwxyz' :\n",
    "#             consonant += 1\n",
    "#     return consonant\n",
    "\n",
    "# def counVocal(word) :\n",
    "#     vocal = 0\n",
    "#     for letter in word :\n",
    "#         if letter in 'aiueo' :\n",
    "#             vocal += 1\n",
    "\n",
    "#     return vocal\n",
    "\n",
    "# def extract_feature(sentence, index) :\n",
    "#     word = sentence[index][0]\n",
    "#     return {\n",
    "#         'word': word,\n",
    "#         'is_first': index == 0,\n",
    "#         'is_last': index == len(sentence) - 1,\n",
    "#         'is_capitalized': word[0].upper() == word[0],\n",
    "#         'is_all_caps': word.upper() == word,\n",
    "#         'is_all_lower': word.lower() == word,\n",
    "#         'prefix-1': word[0],\n",
    "#         'prefix-2': word[:2],\n",
    "#         'prefix-3': word[:3],\n",
    "#         'suffix-1': word[-1],\n",
    "#         'suffix-2': word[-2:],\n",
    "#         'suffix-3': word[-3:],\n",
    "#         'prev_word': '' if index == 0 else sentence[index - 1][0],\n",
    "#         'next_word': '' if index == len(sentence) - 1 else sentence[index + 1][0],\n",
    "#         'has_hyphen': '-' in word,\n",
    "#         'is_numeric': word.isdigit(),\n",
    "#         'consonant_count' : countConsonant(word),\n",
    "#         'vocal_count' : counVocal(word)\n",
    "#     }    \n",
    "\n",
    "# def splitTrainTest(data) :\n",
    "#     cutoff = int(.80 * len(data))\n",
    "#     training_sentences = data[:cutoff]\n",
    "#     test_sentences = data[cutoff:]\n",
    "\n",
    "#     return training_sentences, test_sentences\n",
    "\n",
    "# def data_to_dataframe(datas) :\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     for data in datas :\n",
    "#         for i in range(len(data)) :\n",
    "#             X.append(extract_feature(data, i))\n",
    "#             y.append(data[i][1])\n",
    "    \n",
    "#     return X[:15000], y[:15000]\n",
    "\n",
    "# def train_accuracy() :\n",
    "#     lines = openFile('Indonesian_Manually_Tagged_Corpus.tsv')\n",
    "#     sentences = preprocessData(lines)\n",
    "#     train, test = splitTrainTest(sentences)\n",
    "\n",
    "#     X, y = data_to_dataframe(train)\n",
    "#     X_test, y_test = data_to_dataframe(test)\n",
    "\n",
    "#     v = DictVectorizer(sparse=False)\n",
    "#     X_vect = v.fit_transform(X)\n",
    "#     pickle.dump(v, open(\"post_tag_vectorizer.sav\", \"wb\"))\n",
    "#     X_test_vect = v.transform(X_test)\n",
    "\n",
    "#     model = DecisionTreeClassifier(criterion='entropy')\n",
    "#     model.fit(X_vect, y)\n",
    "\n",
    "#     pickle.dump(model, open(\"post_tag_model.sav\", \"wb\"))\n",
    "#     return accuracy_score(model.predict(X_test_vect), y_test)\n",
    "\n",
    "# if __name__ == \"__main__\" :\n",
    "#     print('Accuracy', train_accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Indonesian_Manually_Tagged_Corpus.tsv', 'r', encoding='utf-8') as tsvfile :\n",
    "    rows = tsvfile.read().split('\\n')\n",
    "    \n",
    "pair = []\n",
    "sentences = []\n",
    "\n",
    "for each in rows:\n",
    "    if each == '':\n",
    "        sentences.append(pair)\n",
    "        pair = [] \n",
    "    else:\n",
    "        word, tag = each.split('\\t')\n",
    "        pair.append([word, tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbor_word(sentence, i):\n",
    "    if i == 0 :\n",
    "        prev_word = ''\n",
    "    else :\n",
    "        prev_word = sentence[i-1][0]\n",
    "    if i == len(sentence)-1 :\n",
    "        next_word = ''\n",
    "    else :\n",
    "        next_word = sentence[i+1][0]\n",
    "    return {'prev_word' : prev_word, 'next_word' : next_word}\n",
    "\n",
    "def morphems(word):\n",
    "    prefix_1 = word[0]\n",
    "    prefix_2 = word[:2]\n",
    "    prefix_3 = word[:3]\n",
    "    prefix_4 = word[:4]\n",
    "    suffix_1 = word[-1]\n",
    "    suffix_2 = word[-2:]\n",
    "    suffix_3 = word[-3:]\n",
    "\n",
    "    return {'prefix_1' : prefix_1, 'prefix_2' : prefix_2, 'prefix_3' : prefix_3, 'prefix_4' : prefix_4, 'suffix_1' : suffix_1, 'suffix_2' : suffix_2, 'suffix_3' : suffix_3}\n",
    "\n",
    "def has_hyphen(word):\n",
    "    return {'has_hyphen' : '-' in word}\n",
    "\n",
    "def is_numeric(word):\n",
    "    return {'is_numeric' : word.isdigit()}\n",
    "\n",
    "def word_case(word):\n",
    "    is_uppercase = word.upper() == word\n",
    "    is_lowercase = word.lower() == word\n",
    "    is_capitalized = word[0].upper() == word[0]\n",
    "    return {'is_uppercase' : is_uppercase,'is_lowercase' : is_lowercase, \n",
    "            'is_capitalized' : is_capitalized}\n",
    "\n",
    "\n",
    "def word_position(sentence, index):\n",
    "    if (index == 0):\n",
    "        pos = 0\n",
    "        prev_pos = -1\n",
    "        next_pos = 1\n",
    "    elif (index == len(sentence)):\n",
    "        pos = 2\n",
    "        prev_pos = 1\n",
    "        next_pos = -1\n",
    "    else :\n",
    "        if(index == 1):\n",
    "            prev_pos = 0\n",
    "            pos = 1\n",
    "            next_pos = 2\n",
    "        elif(index == len(sentence)-1):\n",
    "            prev_pos = 1\n",
    "            pos = 2\n",
    "            next_pos = -1\n",
    "        else :\n",
    "            prev_pos = pos = next_pos = 1\n",
    "    return {'prev_pos' : prev_pos, 'pos' : pos, 'next_pos' : next_pos}\n",
    "\n",
    "def feature_extractor(sentences):\n",
    "    X = []\n",
    "    y = []\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            features = {}\n",
    "\n",
    "            word = sentence[i][0]\n",
    "            features.update({'value': word})\n",
    "            \n",
    "            features.update(neighbor_word(sentence, i))\n",
    "            features.update(word_position(sentence, i))\n",
    "            features.update(morphems(word))\n",
    "            features.update(word_case(word))\n",
    "            features.update(has_hyphen(word))\n",
    "            features.update(is_numeric(word))\n",
    "            \n",
    "            X.append(features)\n",
    "            y.append(sentence[i][1])\n",
    "    return X, y\n",
    "X, y = feature_extractor(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 'untuk',\n",
       " 'prev_word': 'Kera',\n",
       " 'next_word': 'amankan',\n",
       " 'prev_pos': 0,\n",
       " 'pos': 1,\n",
       " 'next_pos': 2,\n",
       " 'prefix_1': 'u',\n",
       " 'prefix_2': 'un',\n",
       " 'prefix_3': 'unt',\n",
       " 'prefix_4': 'untu',\n",
       " 'suffix_1': 'k',\n",
       " 'suffix_2': 'uk',\n",
       " 'suffix_3': 'tuk',\n",
       " 'is_uppercase': False,\n",
       " 'is_lowercase': True,\n",
       " 'is_capitalized': False,\n",
       " 'has_hyphen': False,\n",
       " 'is_numeric': False}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "dict_vect = DictVectorizer(sparse=False)\n",
    "size = 10000\n",
    "y = y[0:size]\n",
    "X = dict_vect.fit_transform(X[0:size])\n",
    "\n",
    "# X1 = dict_vect.fit_transform(X[0:size])\n",
    "# X2 = dict_vect.fit_transform(X[size+1:2*size])\n",
    "\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model_pred = model.predict(X_test)\n",
    "\n",
    "pickle.dump(model, open(\"post_tag_model.sav\", \"wb\"))\n",
    "print('Accuracy Score Logistic : ', accuracy_score(model_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score Logistic :  0.92\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, json, csv\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import pandas as pd, csv\n",
    "import string as string\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterscraper \"(Presiden OR Menteri OR Pemerintahan OR Jokowi) AND (Tolong OR Mohon)\" -l 100 -p 400 -bd 2017-01-01 -ed 2017-06-01 -o dengantolong2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'codecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-29586b524e43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dengantolong2.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlist_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlist_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'codecs' is not defined"
     ]
    }
   ],
   "source": [
    "with codecs.open('dengantolong2.json', 'r', 'utf-8') as f:\n",
    "    tweets = json.load(f, encoding='utf-8')\n",
    "\n",
    "list_tweets = [list(elem.values()) for elem in tweets]\n",
    "list_columns = list(tweets[0].keys())\n",
    "df = pd.DataFrame(list_tweets, columns=list_columns)\n",
    "df.to_csv('dengantolong.csv')\n",
    "tweets[1]\n",
    "\n",
    "list_tweet = []\n",
    "for i in range(len(tweets)):\n",
    "    list_tweet.append([tweets[i]['user'],'1', tweets[i]['text']])\n",
    "    \n",
    "df = pd.DataFrame(list_tweet, columns=['name','class','text'])\n",
    "# df.to_csv('dengantolong.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Indonesian_Manually_Tagged_Corpus.tsv', 'r', encoding='utf-8') as tsvfile :\n",
    "    rows = tsvfile.read().split('\\n')\n",
    "    \n",
    "pair = []\n",
    "sentences = []\n",
    "\n",
    "for each in rows:\n",
    "    if each == '':\n",
    "        sentences.append(pair)\n",
    "        pair = [] \n",
    "    else:\n",
    "        word, tag = each.split('\\t')\n",
    "        pair.append([word, tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbor_word(sentence, i):\n",
    "    if i == 0 :\n",
    "        prev_word = ''\n",
    "    else :\n",
    "        prev_word = sentence[i-1][0]\n",
    "    if i == len(sentence)-1 :\n",
    "        next_word = ''\n",
    "    else :\n",
    "        next_word = sentence[i+1][0]\n",
    "    return {'prev_word' : prev_word, 'next_word' : next_word}\n",
    "\n",
    "def morphems(word):\n",
    "    prefix_1 = word[0]\n",
    "    prefix_2 = word[:2]\n",
    "    prefix_3 = word[:3]\n",
    "    prefix_4 = word[:4]\n",
    "    suffix_1 = word[-1]\n",
    "    suffix_2 = word[-2:]\n",
    "    suffix_3 = word[-3:]\n",
    "\n",
    "    return {'prefix_1' : prefix_1, 'prefix_2' : prefix_2, 'prefix_3' : prefix_3, 'prefix_4' : prefix_4, 'suffix_1' : suffix_1, 'suffix_2' : suffix_2, 'suffix_3' : suffix_3}\n",
    "\n",
    "def has_hyphen(word):\n",
    "    return {'has_hyphen' : '-' in word}\n",
    "\n",
    "def is_numeric(word):\n",
    "    return {'is_numeric' : word.isdigit()}\n",
    "\n",
    "def word_case(word):\n",
    "    is_uppercase = word.upper() == word\n",
    "    is_lowercase = word.lower() == word\n",
    "    is_capitalized = word[0].upper() == word[0]\n",
    "    return {'is_uppercase' : is_uppercase,'is_lowercase' : is_lowercase, \n",
    "            'is_capitalized' : is_capitalized}\n",
    "\n",
    "\n",
    "def word_position(sentence, index):\n",
    "    if (index == 0):\n",
    "        pos = 0\n",
    "        prev_pos = -1\n",
    "        next_pos = 1\n",
    "    elif (index == len(sentence)):\n",
    "        pos = 2\n",
    "        prev_pos = 1\n",
    "        next_pos = -1\n",
    "    else :\n",
    "        if(index == 1):\n",
    "            prev_pos = 0\n",
    "            pos = 1\n",
    "            next_pos = 2\n",
    "        elif(index == len(sentence)-1):\n",
    "            prev_pos = 1\n",
    "            pos = 2\n",
    "            next_pos = -1\n",
    "        else :\n",
    "            prev_pos = pos = next_pos = 1\n",
    "    return {'prev_pos' : prev_pos, 'pos' : pos, 'next_pos' : next_pos}\n",
    "\n",
    "def feature_extractor(sentences):\n",
    "    X = []\n",
    "    y = []\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            features = {}\n",
    "\n",
    "            word = sentence[i][0]\n",
    "            features.update({'value': word})\n",
    "            \n",
    "            features.update(neighbor_word(sentence, i))\n",
    "            features.update(word_position(sentence, i))\n",
    "            features.update(morphems(word))\n",
    "            features.update(word_case(word))\n",
    "            features.update(has_hyphen(word))\n",
    "            features.update(is_numeric(word))\n",
    "            \n",
    "            X.append(features)\n",
    "            y.append(sentence[i][1])\n",
    "    return X, y\n",
    "X, y = feature_extractor(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 'untuk',\n",
       " 'prev_word': 'Kera',\n",
       " 'next_word': 'amankan',\n",
       " 'prev_pos': 0,\n",
       " 'pos': 1,\n",
       " 'next_pos': 2,\n",
       " 'prefix_1': 'u',\n",
       " 'prefix_2': 'un',\n",
       " 'prefix_3': 'unt',\n",
       " 'prefix_4': 'untu',\n",
       " 'suffix_1': 'k',\n",
       " 'suffix_2': 'uk',\n",
       " 'suffix_3': 'tuk',\n",
       " 'is_uppercase': False,\n",
       " 'is_lowercase': True,\n",
       " 'is_capitalized': False,\n",
       " 'has_hyphen': False,\n",
       " 'is_numeric': False}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "dict_vect = DictVectorizer(sparse=False)\n",
    "size = 5000\n",
    "y = y[0:size]\n",
    "X = dict_vect.fit_transform(X[0:size])\n",
    "\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score Logistic :  0.9064\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model_pred = model.predict(X_test)\n",
    "pickle.dump(model, open(\"post_tag_model.sav\", \"wb\"))\n",
    "print('Accuracy Score Logistic : ', accuracy_score(model_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

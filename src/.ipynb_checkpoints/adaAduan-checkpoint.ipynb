{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitterscraper import query_tweets\n",
    "import codecs, json, csv\n",
    "import pandas as pd\n",
    "import random, re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv():\n",
    "    with codecs.open(\"input/twitter_test.json\", 'r', 'utf-8') as f:\n",
    "        tweets = json.load(f, encoding='utf-8')\n",
    "\n",
    "    list_tweets = [list(elem.values()) for elem in tweets]\n",
    "    list_columns = list(tweets[0].keys())\n",
    "    df = pd.DataFrame(list_tweets, columns=list_columns)\n",
    "\n",
    "    list_tweet = []\n",
    "    for i in range(len(tweets)):\n",
    "        list_tweet.append([tweets[i]['user'], tweets[i]['text']])\n",
    "\n",
    "    df = pd.DataFrame(list_tweet, columns=['name', 'text'])\n",
    "    df.to_csv(\"output/twitter_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(data_size):\n",
    "    twitters = pd.read_csv(\"output/twitter_test.csv\")\n",
    "    username = twitters['name']\n",
    "    tweets = twitters['text']\n",
    "\n",
    "    return zip(*random.sample(list(zip(username, tweets)), data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pos_tag():\n",
    "    clf = joblib.load('model/pos_tagger.joblib')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_vectorizer():\n",
    "    clf = joblib.load('model/dict_vectorizer.joblib')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classifier():\n",
    "    clf = joblib.load('model/classifier.joblib')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_count_vectorizer():\n",
    "    clf = joblib.load('model/count_vectorizer.joblib')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twitter(raw):\n",
    "    #remove hashtag, link, and @\n",
    "    cleanr = re.compile(\"http?:\\/\\/.*[\\r\\n]*\")\n",
    "    cleantext = re.sub(cleanr, '', raw)\n",
    "    \n",
    "    cleanr = re.compile(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)*\")\n",
    "    cleantext = re.sub(cleanr, '', cleantext)\n",
    "    \n",
    "    cleantext = re.sub(cleanr, '', cleantext)\n",
    "    \n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(w):\n",
    "    words = word_tokenize(w)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Tolong',\n",
       "  'di',\n",
       "  'Kalimantan',\n",
       "  'Selatan',\n",
       "  'di',\n",
       "  'bantu',\n",
       "  'ekonomi',\n",
       "  'kerakyatan',\n",
       "  'pak',\n",
       "  'serta',\n",
       "  'swasembada',\n",
       "  'pangan',\n",
       "  'Thanks']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_pos_tag(X):\n",
    "    preprocess1 = []\n",
    "    preprocess2 = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        preprocess1.append(preprocess_twitter(X[i]))\n",
    "    \n",
    "    for i in range(len(preprocess1)):\n",
    "        preprocess2.append(tokenizer(preprocess1[i]))\n",
    "    \n",
    "    return preprocess2\n",
    "X = [\"'Tolong di Kalimantan Selatan di bantu ekonomi kerakyatan pak serta swasembada pangan Thanks\"]\n",
    "preprocess_pos_tag(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbor_word(sentence, i):\n",
    "    if i == 0 :\n",
    "        prev_word = ''\n",
    "    else :\n",
    "        prev_word = sentence[i-1]\n",
    "    if i == len(sentence)-1 :\n",
    "        next_word = ''\n",
    "    else :\n",
    "        next_word = sentence[i+1]\n",
    "    return {'prev_word' : prev_word, 'next_word' : next_word}\n",
    "\n",
    "def morphems(word):\n",
    "    prefix_1 = word[0]\n",
    "    prefix_2 = word[:2]\n",
    "    prefix_3 = word[:3]\n",
    "    prefix_4 = word[:4]\n",
    "    suffix_1 = word[-1]\n",
    "    suffix_2 = word[-2:]\n",
    "    suffix_3 = word[-3:]\n",
    "\n",
    "    return {'prefix_1' : prefix_1, 'prefix_2' : prefix_2, 'prefix_3' : prefix_3, 'prefix_4' : prefix_4, 'suffix_1' : suffix_1, 'suffix_2' : suffix_2, 'suffix_3' : suffix_3}\n",
    "\n",
    "def has_hyphen(word):\n",
    "    return {'has_hyphen' : '-' in word}\n",
    "\n",
    "def is_digit(word):\n",
    "    return {'is_digit' : word.isdigit()}\n",
    "\n",
    "def word_case(word):\n",
    "    is_capitalized = word[0].upper() == word[0]\n",
    "    return {'is_capitalized' : is_capitalized}\n",
    "\n",
    "\n",
    "def word_position(sentence, index):\n",
    "    if (index == 0):\n",
    "        pos = 0\n",
    "        prev_pos = -1\n",
    "        next_pos = 1\n",
    "    elif (index == len(sentence)):\n",
    "        pos = 2\n",
    "        prev_pos = 1\n",
    "        next_pos = -1\n",
    "    else :\n",
    "        if(index == 1):\n",
    "            prev_pos = 0\n",
    "            pos = 1\n",
    "            next_pos = 2\n",
    "        elif(index == len(sentence)-1):\n",
    "            prev_pos = 1\n",
    "            pos = 2\n",
    "            next_pos = -1\n",
    "        else :\n",
    "            prev_pos = pos = next_pos = 1\n",
    "    return {'prev_pos' : prev_pos, 'pos' : pos, 'next_pos' : next_pos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(sentences):\n",
    "    X = []\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            features = {}\n",
    "\n",
    "            word = sentence[i]\n",
    "            features.update({'value': word})\n",
    "            \n",
    "            features.update(neighbor_word(sentence, i))\n",
    "            features.update(word_position(sentence, i))\n",
    "            features.update(morphems(word))\n",
    "            features.update(word_case(word))\n",
    "            features.update(has_hyphen(word))\n",
    "            features.update(is_digit(word))\n",
    "            \n",
    "            X.append(features)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_features(features):\n",
    "    dict_vect = load_dict_vectorizer()\n",
    "    features_v = dict_vect.transform(features)\n",
    "    return features_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairing_tag(tweets, tags):\n",
    "    j = 0\n",
    "    pair_tweet = []\n",
    "    for tweet in tweets:\n",
    "        pair = []\n",
    "        for i in range(len(tweet)):\n",
    "            pair.append((tweet[i], tags[j]))\n",
    "            j = j + 1\n",
    "        pair_tweet.append(pair)\n",
    "    return pair_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemWord(words):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "    sentence = ''\n",
    "    for word in words:\n",
    "        sentence += str(stemmer.stem(word)) + ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_classifier(X):\n",
    "    preprocess1 = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        preprocess1.append(stemWord(X[i]))\n",
    "    \n",
    "    return preprocess1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vectorizer(X):\n",
    "    count_vect = load_count_vectorizer()\n",
    "    X = count_vect.transform(X)\n",
    "    return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairing_class(username, tweets, y):\n",
    "    j = 0\n",
    "    pair = []\n",
    "    for tweet in tweets:\n",
    "        pair.append((username[j], tweet, y[j]))\n",
    "        j = j + 1\n",
    "    \n",
    "    return pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairing_all(pengaduan, paired_tweets):\n",
    "    j = 0\n",
    "    pair = []\n",
    "    for tweet in pengaduan:\n",
    "        pair.append((tweet, paired_tweets[j]))\n",
    "        j = j + 1\n",
    "    \n",
    "    return pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_to_csv()\n",
    "\n",
    "username, tweets = load_csv(10)\n",
    "pre_tweets = preprocess_pos_tag(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# username = ['robby', 'kevin12', 'patz']\n",
    "# tweets = ['@Pak Jokowi, mohon tutup tambang emas di Hutan Lindung Tumpang Pitu. http://forbanyuwangi.org/?p=60Â  #Banyuwangi #SaveTumpangPitu', \n",
    "#           'Apa tanggapan bpk @jokowi tentang video tsb ? Tolong di tindak pak....',\n",
    "#           '@km_itb pilih saya menjadi presiden km berikutnya, akan saya turunkan beban sks kailan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_tweets = preprocess_classifier(pre_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aduan {'username': 'joinsputra', 'Tindakan': ['memiliki', 'menindak'], 'Nominal': [], 'Keterangan': ['bayar', 'pajak', 'badan', 'hukum', 'pt', 'mohon', 'kebijakan', 'pak', 'presiden', 'joko', 'widodo', 'utk', 'perbuatan'], 'tweet_lengkap': 'Selama ini bayar pajak & memiliki badan hukum pt, kami mohon atas kebijakan pak presiden joko widodo utk menindak tegas perbuatan itu'}\n",
      "aduan {'username': 'Yayad6874', 'Tindakan': ['ada', 'siap', 'menjalankan'], 'Nominal': ['dua desa'], 'Keterangan': ['Kec', 'Tempat', 'tugas', 'desa', 'yg', 'sistim', 'padat', 'karya', 'cash', 'PD', 'mohon', 'petunjuk', 'teknis', 'petunjuk', 'pelaksaanx', 'pak'], 'tweet_lengkap': 'Di Kec. Tempat sy tugas ada dua desa yg sistim padat karya cash debagai PD siap menjalankan mohon petunjuk teknis dan petunjuk pelaksaanx pak'}\n",
      "aduan {'username': 'cahmeger', 'Tindakan': [], 'Nominal': [], 'Keterangan': ['RI', 'Yth', 'Bapak', 'Presiden', 'Tolong', 'selamatkan', 'anak', 'bangsa', 'pengaruh', 'globalisasi', 'informasi'], 'tweet_lengkap': '@kemkominfo @jokowi @KIPusat @DPR_RI \\r\\nYth Bapak Presiden.  Tolong selamatkan anak bangsa ini dari pengaruh buruk globalisasi informasi.'}\n",
      "aduan {'username': 'tottioke72', 'Tindakan': ['berjuang', 'tertahan'], 'Nominal': ['8 th'], 'Keterangan': ['bangga', 'bapak', 'Jokowiterus', 'bangun', 'negeri', 'cita', 'cita', 'pahlawan', 'utk', 'kemerdekaan', 'negeri', 'itupak', 'Jokowi', 'SDH', 'th', 'sertifikat', 'kantor', 'BPN', 'Semarangmohon', 'bantuannya'], 'tweet_lengkap': 'Kami bangga dengan bapak Jokowi...terus bangun negeri ini\\r\\n seperti cita cita para pahlawan berjuang utk kemerdekaan negeri itu...pak Jokowi SDH 8 th sertifikat kami belum selesai..masih tertahan di di kantor BPN Semarang...mohon bantuannya ....'}\n"
     ]
    }
   ],
   "source": [
    "vector_tweets = word_vectorizer(prep_tweets)\n",
    "\n",
    "classifier = load_classifier()\n",
    "\n",
    "y = classifier.predict(vector_tweets)\n",
    "paired_class = pairing_class(username, tweets, y)\n",
    "\n",
    "\n",
    "pengaduan_list = []\n",
    "pengaduan_tweet = []\n",
    "\n",
    "for each in paired_class:\n",
    "    if (each[2]==1):\n",
    "        pengaduan_list.append(each)\n",
    "        pengaduan_tweet.append(each[1])\n",
    "if len (pengaduan_list) == 0:\n",
    "    print(\"Tidak terdeteksi pengaduan\")\n",
    "    for each in paired_class:\n",
    "        print(each)\n",
    "else:\n",
    "    pre_tweets = preprocess_pos_tag(pengaduan_tweet)\n",
    "    featured_tweets = feature_extractor(pre_tweets)\n",
    "\n",
    "    X = vectorize_features(featured_tweets)\n",
    "    posTagger = load_pos_tag()\n",
    "\n",
    "    tags = posTagger.predict(X)\n",
    "    paired_tweets = pairing_tag(pre_tweets, tags)\n",
    "\n",
    "    pair = pairing_all(pengaduan_list, paired_tweets)\n",
    "    \n",
    "    aduan = []\n",
    "    for each in pair:\n",
    "        Noun = []\n",
    "        Verb = []\n",
    "        Angka = []\n",
    "        for i in range(len(each[1])):\n",
    "            if ((each[1][i][1]=='NN') |( each[1][i][1]=='NNP' )| (each[1][i][1]=='NND') ):\n",
    "                Noun.append(each[1][i][0])\n",
    "            elif (each[1][i][1]=='VB' ):\n",
    "                Verb.append(each[1][i][0])\n",
    "            elif ((each[1][i][1]=='CD' )| (each[1][i][1]=='OD') ):\n",
    "                Angka.append(each[1][i][0] + ' ' + each[1][i+1][0])\n",
    "        detection = ({ 'username' : each[0][0], \n",
    "                     'Tindakan':Verb, 'Nominal': Angka,\n",
    "                     'Keterangan' : Noun, 'tweet_lengkap':each[0][1]})\n",
    "        aduan.append((each[0][0], Verb,  Angka, Noun, each[0][1]))\n",
    "\n",
    "    with open('output/pengaduan.csv', 'w') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(['username', 'Tindakan', 'Nominal', 'Keterangan', 'tweet'])\n",
    "        for each in aduan:\n",
    "            wr.writerow([each[0], each[1], each[2], each[3], each[4]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
